{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02807 Computational Tools for Data Science\n",
    "##### November 28, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import warnings\n",
    "from wordcloud import WordCloud\n",
    "import datetime\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_guardian_article(df_row):\n",
    "    # Get soup\n",
    "    soup = df_row['soup']\n",
    "\n",
    "    # Attempt to get summary\n",
    "    summary_tag = soup.find('div', {'data-gu-name': 'standfirst'})\n",
    "    summary = summary_tag.get_text(strip=True) if summary_tag else None\n",
    "\n",
    "    # Attempt to get author\n",
    "    author_tag = soup.find('a', rel='author')\n",
    "    author = author_tag.get_text(strip=True) if author_tag else 'Unknown'\n",
    "\n",
    "    # Attempt to get content\n",
    "    content_tags = soup.find_all('p')\n",
    "    content = ' '.join(tag.get_text(strip=True) for tag in content_tags[1:])\n",
    "\n",
    "    # Attempt to get date\n",
    "    date_tag = soup.find('span', class_='dcr-u0h1qy')\n",
    "    date_string = date_tag.get_text(strip=True) if date_tag else 'Unknown'\n",
    "\n",
    "    \"\"\"\n",
    "    # Parse the date if it's not 'Unknown'\n",
    "    if date_string != 'Unknown':\n",
    "        try:\n",
    "            date_format = \"%B %d, %Y %I:%M%p\"\n",
    "            date = datetime.strptime(date_string, date_format)\n",
    "        except ValueError:\n",
    "            date = 'Unknown'\n",
    "    else:\n",
    "        date = 'Unknown'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Update the df_row with the scraped data\n",
    "    df_row['summary'] = summary\n",
    "    df_row['author'] = author\n",
    "    df_row['content'] = content\n",
    "    #df_row['date'] = date\n",
    "    df_row['date'] = date_string\n",
    "\n",
    "    return df_row\n",
    "\n",
    "## Scraper function to scrape articles from The Washington Post\n",
    "def scrape_washington_post_article(df_row):\n",
    "    # Get soup\n",
    "    soup = df_row['soup']\n",
    "\n",
    "    # Attempt to get summary\n",
    "    summary = None\n",
    "    summary_candidates = ['PJLV PJLV-iPJLV-css grid-center w-100']\n",
    "    for candidate in summary_candidates:\n",
    "        summary_tag = soup.find(class_=candidate)\n",
    "        if summary_tag:\n",
    "            summary = summary_tag.get_text(strip=True)\n",
    "            break\n",
    "\n",
    "    # Attempt to get author\n",
    "    author = None\n",
    "    author_candidates = ['wpds-c-cNdzuP wpds-c-cNdzuP-ejzZdU-isLink-true', 'a[data-qa=\"author-name\"]']\n",
    "    authors = []\n",
    "    for candidate in author_candidates:\n",
    "        author_tags = soup.find_all(class_=candidate) or soup.find_all('a', {'data-qa': 'author-name'})\n",
    "        for tag in author_tags:\n",
    "            authors.append(tag.get_text(strip=True))\n",
    "    author = ' and '.join(authors)\n",
    "\n",
    "    # Get content\n",
    "    content_list = soup.find_all(class_='article-body grid-center grid-body') or soup.find_all('p')\n",
    "    content = ' '.join(content.get_text(strip=True) for content in content_list)\n",
    "\n",
    "    # Get date\n",
    "    date_string = None\n",
    "    date_candidates = ['wpds-c-iKQyrV', 'wpds-c-iKQyrV wpds-c-iKQyrV-ihqANPJ-css overrideStyles']\n",
    "    for candidate in date_candidates:\n",
    "        date_tag = soup.find(class_=candidate)\n",
    "        if date_tag:\n",
    "            date_string = date_tag.get_text(strip=True)\n",
    "            #date_string = date_string.replace('.m.', 'm').replace('EST', '').strip()\n",
    "            break\n",
    "\n",
    "    # Parse the date\n",
    "    #date_format = \"%B %d, %Y at %I:%M %p\"\n",
    "    #date = datetime.strptime(date_string, date_format) if date_string else None\n",
    "\n",
    "    df_row['summary'] = summary\n",
    "    df_row['author'] = author\n",
    "    df_row['content'] = content\n",
    "    #df_row['date'] = date\n",
    "    df_row['date'] = date_string\n",
    "\n",
    "    return df_row\n",
    "\n",
    "## Scraper function to scrape articles from The New York Post\n",
    "def scrape_ny_post_article(df_row):\n",
    "    # Get soup\n",
    "    soup = df_row['soup']\n",
    "\n",
    "    # No summaries exist\n",
    "    summary = None\n",
    "\n",
    "    # Get author\n",
    "    author_tag = soup.find('span', class_='meta__link')\n",
    "    author = author_tag.get_text(strip=True)[16:] if author_tag else 'Unknown'\n",
    "\n",
    "    # Get content\n",
    "    content_list = soup.find_all('p')[1:-1]\n",
    "    content = ' '.join(content.get_text(strip=True) for content in content_list)\n",
    "\n",
    "    # Get date\n",
    "    date_tag = soup.find('div', class_='date--updated__item')\n",
    "    date_string = date_tag.find_all('span')[1].get_text(strip=True) if date_tag else 'Unknown'\n",
    "    #date_string = date_string.replace('.m.', 'M').replace('ET', '').strip()\n",
    "    #date_format = \"%b. %d, %Y, %I:%M %p\"\n",
    "    #try:\n",
    "        #date = datetime.strptime(date_string, date_format)\n",
    "    #except ValueError:\n",
    "        #date = 'Unknown'\n",
    "\n",
    "    df_row['summary'] = summary\n",
    "    df_row['author'] = author\n",
    "    df_row['content'] = content\n",
    "    #df_row['date'] = date\n",
    "    df_row['date'] = date_string\n",
    "\n",
    "    return df_row\n",
    "\n",
    "## Scraper function to scrape articles from The Atlantic\n",
    "def scrape_atlantic_article(df_row):\n",
    "    # Get soup\n",
    "    soup = df_row['soup']\n",
    "\n",
    "    # Attempt to get summary\n",
    "    summary_tag = soup.find(class_='ArticleHero_dek__EqdkK')\n",
    "    summary = summary_tag.get_text(strip=True) if summary_tag else None\n",
    "\n",
    "    # Attempt to get author\n",
    "    author_tag = soup.find(class_='ArticleBylines_link__kNP4C')\n",
    "    author = author_tag.get_text(strip=True) if author_tag else 'Unknown'\n",
    "\n",
    "    # Attempt to get content\n",
    "    content_tags = soup.find_all('p', class_='ArticleParagraph_root__4mszW')\n",
    "    content = ' '.join(tag.get_text(strip=True) for tag in content_tags)\n",
    "\n",
    "    # Attempt to get date\n",
    "    date_tag = soup.find('time', class_='ArticleTimestamp_root__b3bL6')\n",
    "    date = date_tag['datetime'] if date_tag else 'Unknown'\n",
    "\n",
    "    # Parse the date if it's not 'Unknown'\n",
    "    #if date != 'Unknown':\n",
    "        #try:\n",
    "            #date = datetime.strptime(date, '%Y-%m-%dT%H:%M:%SZ').isoformat()\n",
    "        #except ValueError:\n",
    "            #date = 'Unknown'\n",
    "\n",
    "    # Update the df_row with the scraped data\n",
    "    df_row['summary'] = summary\n",
    "    df_row['author'] = author\n",
    "    df_row['content'] = content\n",
    "    df_row['date'] = date\n",
    "\n",
    "    return df_row\n",
    "\n",
    "## Scraper function to scrape articles from CNN\n",
    "def scrape_cnn_article(df_row):\n",
    "    # Get soup\n",
    "    soup = df_row['soup']\n",
    "\n",
    "    # No summaries for CNN articles\n",
    "    summary = None\n",
    "\n",
    "    # Check if it's a live news article and get the author, content, and date accordingly\n",
    "    if '/live-news/' in df_row['url']:\n",
    "        author_tag = soup.find('p', {'data-type': 'byline-area'})\n",
    "        author = author_tag.get_text(strip=True) if author_tag else 'Unknown'\n",
    "\n",
    "        content_tags = soup.find_all('p', class_='sc-gZMcBi render-stellar-contentstyles__Paragraph-sc-9v7nwy-2 dCwndB')\n",
    "        content = ' '.join(tag.get_text(strip=True) for tag in content_tags)\n",
    "\n",
    "        date_tag = soup.find('div', class_='hJIoKL')\n",
    "        date = date_tag.get_text(strip=True) if date_tag else 'Unknown'\n",
    "        date_string = ' '.join(date.split(' ')[-3:])+' '+str(date.split(' ')[2][1:3])+':'+(date.split(' ')[2][3:5]) if date!='Unknown' else 'Unknown'\n",
    "    else:\n",
    "        author_tag = soup.find(class_='byline__link')\n",
    "        author = author_tag.get_text(strip=True) if author_tag else 'Unknown'\n",
    "\n",
    "        content_tags = soup.find_all('p', class_='paragraph inline-placeholder')\n",
    "        content = ' '.join(tag.get_text(strip=True) for tag in content_tags)\n",
    "\n",
    "        date_tag = soup.find(class_='timestamp')\n",
    "        #date_string = ' '.join(date_tag.get_text(strip=True).split(' ')[-7:][-3:]+date_tag.get_text(strip=True).split(' ')[-7:][0:2]) if date_tag else 'Unknown'\n",
    "        date_string = date_tag.get_text(strip=True).split('\\n')[-1] if date_tag else 'Unknown'\n",
    "\n",
    "    # Parse the date if it's not 'Unknown'\n",
    "    \"\"\"\n",
    "    if date_string != 'Unknown':\n",
    "        try:\n",
    "            if '/live-news/' in df_row['url']:\n",
    "                date_format = \"%B %d, %Y %H:%M\"\n",
    "            else:\n",
    "                date_format = '%B %d, %Y %I:%M %p'\n",
    "            date = datetime.strptime(date_string, date_format).isoformat()\n",
    "        except ValueError:\n",
    "            date = 'Unknown'\n",
    "    \"\"\"\n",
    "\n",
    "    # Update the df_row with the scraped data\n",
    "    df_row['summary'] = summary\n",
    "    df_row['author'] = author\n",
    "    df_row['content'] = content\n",
    "    #df_row['date'] = date\n",
    "    df_row['date'] = date_string\n",
    "\n",
    "    return df_row\n",
    "\n",
    "## Scraper function to scrape articles from Business Insider\n",
    "def scrape_business_insider_article(df_row):\n",
    "    # Get soup\n",
    "    soup = df_row['soup']\n",
    "    \n",
    "    # No summary for BI\n",
    "    summary = None\n",
    "\n",
    "    # Attempt to get author\n",
    "    author_tag = soup.find(class_='byline-author headline-bold')\n",
    "    author = author_tag.get_text(strip=True) if author_tag else 'Unknown'\n",
    "\n",
    "    # Attempt to get content\n",
    "    content_tags = soup.find_all('p')\n",
    "    content = ' '.join(tag.get_text(strip=True) for tag in content_tags[1:])\n",
    "\n",
    "    # Attempt to get date\n",
    "    date_tag = soup.find('div', class_='byline-timestamp')\n",
    "    date = date_tag['data-timestamp'] if date_tag else 'Unknown'\n",
    "\n",
    "    # Update the df_row with the scraped data\n",
    "    df_row['summary'] = summary\n",
    "    df_row['author'] = author\n",
    "    df_row['content'] = content\n",
    "    df_row['date'] = date\n",
    "\n",
    "    return df_row\n",
    "\n",
    "## Scraper function to scrape articles from Fox News\n",
    "def scrape_fox_news_article(df_row):\n",
    "    # Get soup\n",
    "    soup = df_row['soup']\n",
    "\n",
    "    # Attempt to get summary\n",
    "    summary_tag = soup.find('h2', class_='sub-headline speakable')\n",
    "    summary = summary_tag.get_text(strip=True) if summary_tag else None\n",
    "\n",
    "    # Attempt to get author\n",
    "    author_tag = soup.find(class_='author-byline')\n",
    "    author = author_tag.get_text().split('\\n')[-1].replace('Fox News','').strip() if author_tag else 'Unknown'\n",
    "\n",
    "    # Attempt to get content\n",
    "    content_tags = soup.find_all('p')\n",
    "    content = ' '.join(tag.get_text(strip=True) for tag in content_tags[1:])\n",
    "\n",
    "    # Attempt to get date\n",
    "    date_tag = soup.find('time')\n",
    "    #date_string = date_tag.get_text(strip=True)[:-4] if date_tag else 'Unknown'\n",
    "    date_string = date_tag.get_text(strip=True) if date_tag else 'Unknown'\n",
    "\n",
    "    \"\"\"\n",
    "    # Parse the date if it's not 'Unknown'\n",
    "    if date_string != 'Unknown':\n",
    "        try:\n",
    "            date_format = \"%B %d, %Y %I:%M%p\"\n",
    "            date = datetime.strptime(date_string, date_format)\n",
    "        except ValueError:\n",
    "            date = 'Unknown'\n",
    "    else:\n",
    "        date = 'Unknown'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Update the df_row with the scraped data\n",
    "    df_row['summary'] = summary\n",
    "    df_row['author'] = author\n",
    "    df_row['content'] = content\n",
    "    #df_row['date'] = date\n",
    "    df_row['date'] = date_string\n",
    "\n",
    "    return df_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Top_News:\n",
    "    # Class initializer with publication names and setup for scraping functions and class data\n",
    "    def __init__(self, publication_names):\n",
    "        self.publication_names = publication_names if publication_names != 'all' else ['NY Post', 'Atlantic', 'CNN', 'Business Insider', 'Washington Post', 'Fox News', 'Guardian']\n",
    "        self.results_df = pd.DataFrame()\n",
    "        # Dictionary containing scraping function for each publication\n",
    "        self.scrapers = {\n",
    "            'NY Post': scrape_ny_post_article,\n",
    "            'Atlantic': scrape_atlantic_article,\n",
    "            'CNN': scrape_cnn_article,\n",
    "            'Business Insider': scrape_business_insider_article,\n",
    "            'Washington Post': scrape_washington_post_article,\n",
    "            'Fox News': scrape_fox_news_article,\n",
    "            'Guardian': scrape_guardian_article\n",
    "        }\n",
    "        # Dictionary containing class data for each publication to locate articles on their homepage\n",
    "        self.class_data = {\n",
    "            'NY Post': ['https://nypost.com/', 'story__headline headline headline--xl', 'story__headline headline headline--sm', 'story__headline headline headline--combo-lg-xl headline--with-inline-webwood'],\n",
    "            'Atlantic': ['https://www.theatlantic.com/world/', 'HomepageBottom_channelArticle__2wxRe', 'SmallPromoItem_root__nkm_2', 'Lede_title__7Wg1g', 'Offlede_title__kiinC', 'QuadBelt_title__mB6Zf', 'DoubleWide_title__diUPi', 'DoubleStack_title___FhPb', 'Latest_article__DW75m', 'Popular_listItem__CtMMj'],\n",
    "            'CNN': ['https://edition.cnn.com/', 'container__link', 'container__title container_lead-package__title container__title--emphatic hover container__title--emphatic-size-l1'],\n",
    "            'Business Insider': ['https://www.businessinsider.com/', 'tout', 'quick-link', 'most-popular-item', '.featured-tout-collection-wrapper .tout-title a', '.two-column-wrapper .tout-title-link'],\n",
    "            'Washington Post': ['https://www.washingtonpost.com/', 'wpds-c-iiQaMf wpds-c-iiQaMf-igUpeQR-css', 'wpds-c-iiQaMf wpds-c-iiQaMf-ikZTsyd-css', 'wpds-c-iiQaMf wpds-c-iiQaMf-ibYgSwf-css'],\n",
    "            'Fox News': ['https://www.foxnews.com/', 'article'],\n",
    "            'Guardian': ['https://www.theguardian.com/', 'dcr-12ilguo', 'dcr-yw3hn9']\n",
    "        }\n",
    "    \n",
    "    def _get_soup(self, url,rate_limit_seconds=1):\n",
    "        try:\n",
    "            # implement rate limiting for ethical reasons\n",
    "            time.sleep(rate_limit_seconds)\n",
    "            # try to get response from server and parse it into a BeautifulSoup object\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  \n",
    "            return BeautifulSoup(response.text, 'html.parser')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # handle exceptions\n",
    "            print(f\"Request failed for {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_publications(self):\n",
    "        # use class data to find and parse articles from homepages\n",
    "        selected_class_data = {name: self.class_data[name] for name in self.publication_names if name in self.class_data}\n",
    "        articles_data = []\n",
    "           \n",
    "        for class_name in selected_class_data:\n",
    "            # find articles and collect data\n",
    "            base_url = selected_class_data[class_name][0]\n",
    "            soup = self._get_soup(base_url)\n",
    "            \n",
    "            # find article blocks\n",
    "            article_blocks = []\n",
    "            for c_ in selected_class_data[class_name][1:]:\n",
    "                temp = soup.find_all(class_=c_) if (class_name != 'Fox News') else soup.find_all('article')\n",
    "                temp = temp if temp else soup.select(c_)\n",
    "                article_blocks.extend(temp)\n",
    "            \n",
    "            # extract data from each block\n",
    "            for block in article_blocks:\n",
    "                url = block.find('a')['href'] if block.find('a') else (block['href'] if 'href' in block.attrs else None)\n",
    "                if url:\n",
    "                    article_data = {\n",
    "                        'name': class_name,\n",
    "                        'title': block.get_text(strip=True),\n",
    "                        'url': base_url[:-1]+url if url.startswith('/') else url,\n",
    "                        'soup': self._get_soup(url) # get content of article\n",
    "                    }\n",
    "                    articles_data.append(article_data)\n",
    "\n",
    "        self.results_df = pd.DataFrame(articles_data)\n",
    "        # drop duplicates\n",
    "        self.results_df.drop_duplicates(subset=['url'], inplace=True)\n",
    "        self.results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return self.results_df\n",
    "\n",
    "    def article_distribution(self):\n",
    "        if not self.results_df.empty:\n",
    "            # plot distribution of articles from different publishers\n",
    "            self.results_df.groupby('name').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
    "            plt.gca().spines[['top', 'right',]].set_visible(False)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('Scrape publications first')\n",
    "\n",
    "    def scrape_articles(self, max_articles_per_publication=None):\n",
    "        if self.results_df.empty:\n",
    "            print(\"Scraping publications for article URLs...\")\n",
    "            self.scrape_publications()\n",
    "\n",
    "        all_article_data = []\n",
    "        articles_count = {}\n",
    "\n",
    "        for index, row in self.results_df.iterrows():\n",
    "            publication_name = row['name']\n",
    "            if publication_name in self.scrapers:\n",
    "                articles_count.setdefault(publication_name, 0)\n",
    "                # do not scrape more than the desired number of articles per publisher\n",
    "                if max_articles_per_publication is None or articles_count[publication_name] < max_articles_per_publication:\n",
    "                    try:\n",
    "                        article_data = self.scrapers[publication_name](row)\n",
    "                        all_article_data.append(article_data)\n",
    "                        articles_count[publication_name] += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"An error occurred while scraping {row['url']}: {e}\")\n",
    "\n",
    "        self.results_df = pd.DataFrame(all_article_data)\n",
    "        self.results_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(filename='scraping_data.csv', publication_list='all', max_limit_num_articles=None):\n",
    "    news_scraper = Top_News(publication_list)\n",
    "    news_scraper.scrape_publications()\n",
    "    #news_scraper.article_distribution()\n",
    "    news_scraper.scrape_articles(max_articles_per_publication=max_limit_num_articles)\n",
    "    filename = os.path.join('./data/', filename)\n",
    "    news_scraper.results_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape the web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select_agencies = ['NY Post','Atlantic','CNN','Business Insider','Washington Post','Fox News','Guardian']\n",
    "# number_of_articles = 10\n",
    "# scraper(filename=\"new_scraping_data.csv\", publication_list=select_agencies, max_limit_num_articles=int(number_of_articles))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        try:\n",
    "            nltk.download('stopwords') # Download necessary NLTK resources to remove stop words\n",
    "        except nltk.exceptions.AlreadyDownloaded:\n",
    "            # Handle the case where the resource is already downloaded\n",
    "            pass\n",
    "    def preprocess_articles(self, text):\n",
    "        text = text.lower() # Convert data to lover case to remove multiple occurences of the same word\n",
    "        text = re.sub(r'[^a-z\\s]', '', text) # Remove special characters, digits and white space using regex expression\n",
    "        stop_words = set(stopwords.words('english')) # Import stop words from nltk library\n",
    "        words = [word for word in text.split() if word not in stop_words] # Extract all words that are not stop words\n",
    "        stemmer = PorterStemmer() # Apply stemming using nltk PorterStemmer\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]\n",
    "        return ' '.join(stemmed_words)\n",
    "\n",
    "    # Create vocabulary of words from news articles\n",
    "    def vocabulary(self, docs):\n",
    "        vocab = set()\n",
    "        for doc in docs:\n",
    "            for word in doc:\n",
    "                vocab.add(word)\n",
    "        return sorted(vocab)\n",
    "\n",
    "    # Calculate Term frequency\n",
    "    def term_frequency(self, docs, vocab):\n",
    "        tf = pd.DataFrame(0, index=range(len(docs)), columns=vocab)\n",
    "        for i, document in enumerate(docs):\n",
    "            num_words_doc = len(document)\n",
    "            for word in document:\n",
    "                tf.at[i, word] += document.count(word)/num_words_doc\n",
    "        return tf\n",
    "\n",
    "    # Inverse Document Frequency\n",
    "    def inverse_document_frequency(self, docs, vocab):\n",
    "        # We want to reduce the weight of terms that appear frequently in our collection of articles.\n",
    "        idf = pd.Series(0, index=vocab) # Create series and set all elements to 0\n",
    "        for word in vocab:\n",
    "            counter = 0\n",
    "            for doc in docs:\n",
    "                if word in doc:\n",
    "                    counter +=1\n",
    "            idf[word] = np.log((len(docs))/(counter+1)) #TFIDF as stated in the slides of week 1\n",
    "        return idf\n",
    "    \n",
    "    # TF-IDF\n",
    "    def tf_idf(self, tf, idf, docs, vocab):\n",
    "        tfidf = pd.DataFrame(index=range(len(docs)), columns=vocab)  # Create an empty DataFrame\n",
    "        for i in range(len(docs)):\n",
    "            tfidf.iloc[i] = tf.iloc[i]*idf  # Multiply TF values by IDF for each term\n",
    "        tfidf = normalize(tfidf, norm='l2', axis=1)  # L2 normalization to scale vectors\n",
    "        tfidf = pd.DataFrame(tfidf, columns=vocab)\n",
    "        return tfidf\n",
    "\n",
    "    def fit(self):\n",
    "        # Apply preprocesing to news articles\n",
    "        self.df['preprocessed_content'] = self.df['content'].apply(self.preprocess_articles)\n",
    "        # Extract words from each news articles\n",
    "        docs = self.df['preprocessed_content'].str.split()\n",
    "        # Create a vocabulary corresponding to all the words in every news article\n",
    "        vocab = self.vocabulary(docs)\n",
    "        # Calculate the TF\n",
    "        tf = self.term_frequency(docs, vocab)\n",
    "        # Calculate the IDF\n",
    "        idf = self.inverse_document_frequency(docs, vocab)\n",
    "        # Multiply TF with IDF and calculate TF-IDF\n",
    "        tfidf = self.tf_idf(tf, idf, docs, vocab)\n",
    "        return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansAlgorithm:\n",
    "\n",
    "    def __init__(self, X, k, max_iterations=100):\n",
    "      self.X = X.values\n",
    "      self.k = k\n",
    "      self.max_iterations = max_iterations\n",
    "      self.centroids = None\n",
    "      self.num_articles = X.shape[0]\n",
    "\n",
    "    def init_random_centroids(self):\n",
    "      # Initializing KMeans by choosing random centroids \n",
    "      idx = np.random.choice(self.num_articles, size=self.k, replace=False) # Extract random indices from dataframe\n",
    "      centroids = self.X[idx] # Pick random rows from the dataframe\n",
    "      return centroids\n",
    "\n",
    "    def calculate_euclidean_distances(self):\n",
    "      # Calculate distances from vector/row to centroid\n",
    "      num_centroids = self.centroids.shape[0]\n",
    "      distances = np.zeros((num_centroids, self.num_articles))\n",
    "\n",
    "      for centroid_idx in range(num_centroids):\n",
    "          for article_idx in range(self.num_articles):\n",
    "              distances[centroid_idx, article_idx] = np.sqrt(np.sum((self.centroids[centroid_idx, :] - self.X[article_idx, :]) ** 2))\n",
    "      return distances\n",
    "\n",
    "    def update_centroids(self, labels):\n",
    "        # Calculate the mean of each cluster as new centroid\n",
    "        new_centroids = []\n",
    "        for k in range(self.k):\n",
    "            mean = self.X[labels == k].mean(axis=0)\n",
    "            new_centroids.append(mean)\n",
    "        new_centroids = np.array(new_centroids)\n",
    "        return new_centroids\n",
    "\n",
    "    def plot_clusters(self, labels, centroids, iteration):\n",
    "        # We will use PCA to plots clusters as the TFIDF matrix has many dimensions\n",
    "        unique_labels = np.unique(labels)\n",
    "        pca = PCA(n_components=3)\n",
    "        data_3d = pca.fit_transform(self.X)\n",
    "        centroids_3d = pca.transform(centroids)\n",
    "        # 3D plot\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        color_map = cm.get_cmap('turbo', len(unique_labels))\n",
    "        # Plot data\n",
    "        for label in unique_labels:\n",
    "            indices = labels == label # like [1, 2, 1, 3, 1]\n",
    "            ax.scatter(data_3d[indices, 0], data_3d[indices, 1], data_3d[indices, 2], label=f'Cluster {label}', c=[color_map(label)])\n",
    "        # Plot centroids\n",
    "        ax.scatter(centroids_3d[:, 0], centroids_3d[:, 1], centroids_3d[:, 2], c='red', marker='x', s=80, label='Centroids')\n",
    "        ax.set_title(f'PCA 3D - iteration {iteration}')\n",
    "        ax.set_xlabel('Principal component 1')\n",
    "        ax.set_ylabel('Principal component 2')\n",
    "        ax.set_zlabel('Principal component 3')\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
    "        clear_output(wait=True)\n",
    "        plt.show()\n",
    "\n",
    "    def fit(self):\n",
    "      print(\"Clustering: KMeans fit()\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "      \n",
    "      # Run the Kmeans algorithm using helper functions\n",
    "      self.centroids = self.init_random_centroids() # Init centroids\n",
    "\n",
    "      for i in range(self.max_iterations):\n",
    "        distances = self.calculate_euclidean_distances() # calculate eucledian distances\n",
    "        labels = np.argmin(distances, axis=0) # Assign to the cluster with the centroid that has the minimum distance to that point\n",
    "        new_centroids = self.update_centroids(labels) # Calculated centroids based on mean of the points in that cluster\n",
    "\n",
    "        if np.all(new_centroids == self.centroids): # If no new centroids break loop\n",
    "          print(\"Clustering: Kmeans has converged!\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "          break\n",
    "\n",
    "        self.centroids = new_centroids\n",
    "        self.plot_clusters(labels, self.centroids, i) # Plot PCA 3D plot\n",
    "\n",
    "      return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_documents(df):\n",
    "    \"\"\"\n",
    "    Main TF-IDF Function\n",
    "    Total time for 500 articles: 20 minutes\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): dataframe with preprocessed content\n",
    "        \n",
    "    Returns:\n",
    "        tf idf matrix\n",
    "    \"\"\"\n",
    "    tfidf = TFIDF(df)\n",
    "    tfidf_matrix = tfidf.fit()\n",
    "\n",
    "    return tfidf_matrix\n",
    "\n",
    "def Cluster_Articles(k, tfidf_matrix):\n",
    "    tfidf_matrix = extract_documents(tfidf_matrix)\n",
    "    Kmeans = KMeansAlgorithm(tfidf_matrix, k) # Optimal number of clusters is determined from the results of the elbow method\n",
    "    labels = Kmeans.fit()\n",
    "    tfidf_matrix['cluster'] = labels\n",
    "    return tfidf_matrix\n",
    "\n",
    "def elbow_method(data):\n",
    "    # We will use elbow method to determine optimal number of clusters\n",
    "    num_clusters = range(1, 30)\n",
    "    wcss = []\n",
    "\n",
    "    for k in num_clusters:\n",
    "        # We will use Sklearn's Kmeans algorithm, but use random intialization as used in our implementation\n",
    "        kmeans = KMeans(n_clusters=k, init='random', random_state=123)\n",
    "        kmeans.fit(data)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "    # Plot the Elbow Method\n",
    "    plt.plot(num_clusters, wcss, marker='o')\n",
    "    plt.title('Elbow Method - KMeans with random initialization')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Wcss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "First 20 entries in data (size 50):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17283</td>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17284</td>\n",
       "      <td>Rift Between Officers and Residents as Killing...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Benjamin Mueller and Al Baker</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>After the bullet shells get counted, the blood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17285</td>\n",
       "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Margalit Fox</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17286</td>\n",
       "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>William McDonald</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17287</td>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Choe Sang-Hun</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17288</td>\n",
       "      <td>Sick With a Cold, Queen Elizabeth Misses New Y...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Sewell Chan</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>LONDON  —   Queen Elizabeth II, who has been b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17289</td>\n",
       "      <td>Taiwan’s President Accuses China of Renewed In...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Javier C. Hernández</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>BEIJING  —   President Tsai   of Taiwan sharpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17290</td>\n",
       "      <td>After ‘The Biggest Loser,’ Their Bodies Fought...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Gina Kolata</td>\n",
       "      <td>2017-02-08</td>\n",
       "      <td>Danny Cahill stood, slightly dazed, in a blizz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17291</td>\n",
       "      <td>First, a Mixtape. Then a Romance. - The New Yo...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Katherine Rosman</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>Just how   is Hillary Kerr, the    founder of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17292</td>\n",
       "      <td>Calling on Angels While Enduring the Trials of...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Andy Newman</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>Angels are everywhere in the Muñiz family’s ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17293</td>\n",
       "      <td>Weak Federal Powers Could Limit Trump’s Climat...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Justin Gillis</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>With Donald J. Trump about to take control of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17294</td>\n",
       "      <td>Can Carbon Capture Technology Prosper Under Tr...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>John Schwartz</td>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>THOMPSONS, Tex.  —   Can one of the most promi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17295</td>\n",
       "      <td>Mar-a-Lago, the Future Winter White House and ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Maggie Haberman</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>WEST PALM BEACH, Fla.  —   When   Donald J. Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>17296</td>\n",
       "      <td>How to form healthy habits in your 20s - The N...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Charles Duhigg</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>This article is part of a series aimed at help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17297</td>\n",
       "      <td>Turning Your Vacation Photos Into Works of Art...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Stephanie Rosenbloom</td>\n",
       "      <td>2017-04-14</td>\n",
       "      <td>It’s the season for family travel and photos  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17298</td>\n",
       "      <td>As Second Avenue Subway Opens, a Train Delay E...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Emma G. Fitzsimmons</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>Finally. The Second Avenue subway opened in Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17300</td>\n",
       "      <td>Dylann Roof Himself Rejects Best Defense Again...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Kevin Sack and Alan Blinder</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>pages into the   journal found in Dylann S. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17301</td>\n",
       "      <td>Modi’s Cash Ban Brings Pain, but Corruption-We...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Geeta Anand</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>MUMBAI, India  —   It was a bold and risky gam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17302</td>\n",
       "      <td>Suicide Bombing in Baghdad Kills at Least 36 -...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>The Associated Press</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>BAGHDAD  —   A suicide bomber detonated a pick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>17303</td>\n",
       "      <td>Fecal Pollution Taints Water at Melbourne’s Be...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Brett Cole</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>SYDNEY, Australia  —   The annual beach pilgri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              title     publication  \\\n",
       "0   17283  House Republicans Fret About Winning Their Hea...  New York Times   \n",
       "1   17284  Rift Between Officers and Residents as Killing...  New York Times   \n",
       "2   17285  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...  New York Times   \n",
       "3   17286  Among Deaths in 2016, a Heavy Toll in Pop Musi...  New York Times   \n",
       "4   17287  Kim Jong-un Says North Korea Is Preparing to T...  New York Times   \n",
       "5   17288  Sick With a Cold, Queen Elizabeth Misses New Y...  New York Times   \n",
       "6   17289  Taiwan’s President Accuses China of Renewed In...  New York Times   \n",
       "7   17290  After ‘The Biggest Loser,’ Their Bodies Fought...  New York Times   \n",
       "8   17291  First, a Mixtape. Then a Romance. - The New Yo...  New York Times   \n",
       "9   17292  Calling on Angels While Enduring the Trials of...  New York Times   \n",
       "10  17293  Weak Federal Powers Could Limit Trump’s Climat...  New York Times   \n",
       "11  17294  Can Carbon Capture Technology Prosper Under Tr...  New York Times   \n",
       "12  17295  Mar-a-Lago, the Future Winter White House and ...  New York Times   \n",
       "13  17296  How to form healthy habits in your 20s - The N...  New York Times   \n",
       "14  17297  Turning Your Vacation Photos Into Works of Art...  New York Times   \n",
       "15  17298  As Second Avenue Subway Opens, a Train Delay E...  New York Times   \n",
       "16  17300  Dylann Roof Himself Rejects Best Defense Again...  New York Times   \n",
       "17  17301  Modi’s Cash Ban Brings Pain, but Corruption-We...  New York Times   \n",
       "18  17302  Suicide Bombing in Baghdad Kills at Least 36 -...  New York Times   \n",
       "19  17303  Fecal Pollution Taints Water at Melbourne’s Be...  New York Times   \n",
       "\n",
       "                           author       date  \\\n",
       "0                      Carl Hulse 2016-12-31   \n",
       "1   Benjamin Mueller and Al Baker 2017-06-19   \n",
       "2                    Margalit Fox 2017-01-06   \n",
       "3                William McDonald 2017-04-10   \n",
       "4                   Choe Sang-Hun 2017-01-02   \n",
       "5                     Sewell Chan 2017-01-02   \n",
       "6             Javier C. Hernández 2017-01-02   \n",
       "7                     Gina Kolata 2017-02-08   \n",
       "8                Katherine Rosman 2016-12-31   \n",
       "9                     Andy Newman 2016-12-31   \n",
       "10                  Justin Gillis 2017-01-03   \n",
       "11                  John Schwartz 2017-01-05   \n",
       "12                Maggie Haberman 2017-01-02   \n",
       "13                 Charles Duhigg 2017-01-02   \n",
       "14           Stephanie Rosenbloom 2017-04-14   \n",
       "15            Emma G. Fitzsimmons 2017-01-02   \n",
       "16    Kevin Sack and Alan Blinder 2017-01-02   \n",
       "17                    Geeta Anand 2017-01-02   \n",
       "18           The Associated Press 2017-01-03   \n",
       "19                     Brett Cole 2017-01-03   \n",
       "\n",
       "                                              content  \n",
       "0   WASHINGTON  —   Congressional Republicans have...  \n",
       "1   After the bullet shells get counted, the blood...  \n",
       "2   When Walt Disney’s “Bambi” opened in 1942, cri...  \n",
       "3   Death may be the great equalizer, but it isn’t...  \n",
       "4   SEOUL, South Korea  —   North Korea’s leader, ...  \n",
       "5   LONDON  —   Queen Elizabeth II, who has been b...  \n",
       "6   BEIJING  —   President Tsai   of Taiwan sharpl...  \n",
       "7   Danny Cahill stood, slightly dazed, in a blizz...  \n",
       "8   Just how   is Hillary Kerr, the    founder of ...  \n",
       "9   Angels are everywhere in the Muñiz family’s ap...  \n",
       "10  With Donald J. Trump about to take control of ...  \n",
       "11  THOMPSONS, Tex.  —   Can one of the most promi...  \n",
       "12  WEST PALM BEACH, Fla.  —   When   Donald J. Tr...  \n",
       "13  This article is part of a series aimed at help...  \n",
       "14  It’s the season for family travel and photos  ...  \n",
       "15  Finally. The Second Avenue subway opened in Ne...  \n",
       "16    pages into the   journal found in Dylann S. ...  \n",
       "17  MUMBAI, India  —   It was a bold and risky gam...  \n",
       "18  BAGHDAD  —   A suicide bomber detonated a pick...  \n",
       "19  SYDNEY, Australia  —   The annual beach pilgri...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset_paths = [file for file in os.listdir(\"./data/\") if file.endswith(('csv'))] # Select all available data files\n",
    "# dataset_paths = [\"scraping_data_filtered.csv\"]\n",
    "dataset_paths = [\"articles1.csv\"]\n",
    "\n",
    "# Load data\n",
    "data_frames = []\n",
    "data = []\n",
    "DATA_LIMIT = 50 #520 # how many items to load\n",
    "for data_path in dataset_paths:\n",
    "    # data_frame_temp = pd.read_csv(os.path.join('./data/', data_path)).head(DATA_LIMIT)\n",
    "    # print(data_frame_temp.shape[0])\n",
    "    # for i in range(data_frame_temp.shape[0]):\n",
    "        #   data_frame_temp.loc[i,'content'] = str(data_frame_temp.loc[i,'content'])\n",
    "    # data_frame_temp['content'] = data_frame_temp['content'].astype(str)\n",
    "    data_frames.append(pd.read_csv(os.path.join('./data/', data_path)).head(DATA_LIMIT))\n",
    "    # for i in range(data)\n",
    "    # data_frames['content'] = data_frames['content'].astype(str)\n",
    "    # data_frames.append(data_frame_temp)\n",
    "data = pd.concat(data_frames)\n",
    "print(type(data))\n",
    "data['date'] = pd.to_datetime(data['date'],format='mixed')\n",
    "data = data.drop(['Unnamed: 0', 'year', 'month', 'url'], axis=1, errors='ignore')\n",
    "\n",
    "if len(data) > 0:\n",
    "        # display dataframe header\n",
    "        print(\"First 20 entries in data (size \" + str(len(data)) + \"):\")\n",
    "        display(data.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Similar Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cluster_Articles() got an unexpected keyword argument 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\MyDocs\\DTU\\MSc\\Courses\\Computational Tools for Data Science\\Project\\github\\NewsComparison\\Jupyter_notebook_handin.ipynb Cell 18\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/MyDocs/DTU/MSc/Courses/Computational%20Tools%20for%20Data%20Science/Project/github/NewsComparison/Jupyter_notebook_handin.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m test \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/MyDocs/DTU/MSc/Courses/Computational%20Tools%20for%20Data%20Science/Project/github/NewsComparison/Jupyter_notebook_handin.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(test))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/MyDocs/DTU/MSc/Courses/Computational%20Tools%20for%20Data%20Science/Project/github/NewsComparison/Jupyter_notebook_handin.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m data \u001b[39m=\u001b[39m Cluster_Articles(k\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(nr_clusters), data\u001b[39m=\u001b[39;49mdata\u001b[39m.\u001b[39;49mcopy())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/MyDocs/DTU/MSc/Courses/Computational%20Tools%20for%20Data%20Science/Project/github/NewsComparison/Jupyter_notebook_handin.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m data:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/MyDocs/DTU/MSc/Courses/Computational%20Tools%20for%20Data%20Science/Project/github/NewsComparison/Jupyter_notebook_handin.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# display dataframe header\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/MyDocs/DTU/MSc/Courses/Computational%20Tools%20for%20Data%20Science/Project/github/NewsComparison/Jupyter_notebook_handin.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFirst 20 entries in data with clusters:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Cluster_Articles() got an unexpected keyword argument 'data'"
     ]
    }
   ],
   "source": [
    "nr_clusters = 8 # Number of clusters\n",
    "    \n",
    "clustered_data = pd.DataFrame()\n",
    "\n",
    "test = data.copy()\n",
    "print(type(test))\n",
    "\n",
    "data = Cluster_Articles(k=int(nr_clusters), data=data.copy())\n",
    "    \n",
    "if 'cluster' in data:\n",
    "    # display dataframe header\n",
    "    print(\"First 20 entries in data with clusters:\")\n",
    "    data[['title', 'cluster']].head(5)\n",
    "    \n",
    "    # Save clusters\n",
    "    # clustered_path = text_area(\"Path\", value=\"clustered_articles1.csv\")\n",
    "    # data.to_csv(os.path.join(\"data/\", str(clustered_path)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
