{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8tSqfiYsHaj"
      },
      "source": [
        "# 02807 Computational Tools for Data Science\n",
        "##### November 28, 2023\n",
        "__*Authors*__\n",
        "- Vidisha Sinha - s204081\n",
        "- David van Scheppingen - s222902\n",
        "- Ari Goldhar Menachem - s163956\n",
        "- Magnus Nikolaj Nyholm Jensen - s184677\n",
        "***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z0maGH-9sHao"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "from matplotlib import cm, ticker\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.colors import ListedColormap\n",
        "from IPython.display import clear_output\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import warnings\n",
        "from wordcloud import WordCloud\n",
        "from datetime import datetime\n",
        "from datetime import datetime, timedelta\n",
        "import dateutil.parser\n",
        "import seaborn as sns\n",
        "import time\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkmLRkNlsHa0"
      },
      "source": [
        "### TF-IDF class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TFIDF:\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        try:\n",
        "            nltk.download('stopwords') # Download necessary NLTK resources to remove stop words\n",
        "        except nltk.exceptions.AlreadyDownloaded:\n",
        "            # Handle the case where the resource is already downloaded\n",
        "            pass\n",
        "    def preprocess_articles(self, text):\n",
        "        text = text.lower() # Convert data to lover case to remove multiple occurences of the same word\n",
        "        text = re.sub(r'[^a-z\\s]', '', text) # Remove special characters, digits and white space using regex expression\n",
        "        stop_words = set(stopwords.words('english')) # Import stop words from nltk library\n",
        "        words = [word for word in text.split() if word not in stop_words] # Extract all words that are not stop words\n",
        "        stemmer = PorterStemmer() # Apply stemming using nltk PorterStemmer\n",
        "        stemmed_words = [stemmer.stem(word) for word in words]\n",
        "        return ' '.join(stemmed_words)\n",
        "\n",
        "    def preprocess_text_lowercase(self, text):\n",
        "        # Converts to lowercase, removes symbols and stopword, but does not stem\n",
        "        # Used for wordclouds\n",
        "        text = text.lower()\n",
        "        # Remove special characters, digits and white space using regex expression\n",
        "        text = re.sub(r'[^a-z\\s]', '', text)\n",
        "        # Tokenize and remove stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in text.split() if word not in stop_words]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    # Create vocabulary of words from news articles\n",
        "    def vocabulary(self, docs):\n",
        "        vocab = set()\n",
        "        for doc in docs:\n",
        "            for word in doc:\n",
        "                vocab.add(word)\n",
        "        return sorted(vocab)\n",
        "\n",
        "    # Calculate Term frequency\n",
        "    def term_frequency(self, docs, vocab):\n",
        "        tf = pd.DataFrame(0, index=range(len(docs)), columns=vocab)\n",
        "        for i, document in enumerate(docs):\n",
        "            num_words_doc = len(document)\n",
        "            for word in document:\n",
        "                tf.at[i, word] += document.count(word)/num_words_doc\n",
        "        return tf\n",
        "\n",
        "    # Inverse Document Frequency\n",
        "    def inverse_document_frequency(self, docs, vocab):\n",
        "        # We want to reduce the weight of terms that appear frequently in our collection of articles.\n",
        "        idf = pd.Series(0, index=vocab) # Create series and set all elements to 0\n",
        "        for word in vocab:\n",
        "            counter = 0\n",
        "            for doc in docs:\n",
        "                if word in doc:\n",
        "                    counter +=1\n",
        "            idf[word] = np.log((len(docs))/(counter+1)) #TFIDF as stated in the slides of week 1\n",
        "        return idf\n",
        "\n",
        "    # TF-IDF\n",
        "    def tf_idf(self, tf, idf, docs, vocab):\n",
        "        tfidf = pd.DataFrame(index=range(len(docs)), columns=vocab)  # Create an empty DataFrame\n",
        "        for i in range(len(docs)):\n",
        "            tfidf.iloc[i] = tf.iloc[i]*idf  # Multiply TF values by IDF for each term\n",
        "        tfidf = normalize(tfidf, norm='l2', axis=1)  # L2 normalization to scale vectors\n",
        "        tfidf = pd.DataFrame(tfidf, columns=vocab)\n",
        "        return tfidf\n",
        "\n",
        "    def fit(self):\n",
        "        # Apply preprocesing to news articles\n",
        "        self.df['preprocessed_content'] = self.df['content'].apply(self.preprocess_articles)\n",
        "        self.df['content_lowercase'] = self.df['content'].apply(self.preprocess_text_lowercase)\n",
        "        content_lowercase_dataframe = self.df['content_lowercase'] # pd.DataFrame()\n",
        "        content_lowercase_dataframe = pd.DataFrame(content_lowercase_dataframe)\n",
        "        # Extract words from each news articles\n",
        "        docs = self.df['preprocessed_content'].str.split()\n",
        "        # Create a vocabulary corresponding to all the words in every news article\n",
        "        vocab = self.vocabulary(docs)\n",
        "        # Calculate the TF\n",
        "        tf = self.term_frequency(docs, vocab)\n",
        "        # Calculate the IDF\n",
        "        idf = self.inverse_document_frequency(docs, vocab)\n",
        "        # Multiply TF with IDF and calculate TF-IDF\n",
        "        tfidf = self.tf_idf(tf, idf, docs, vocab)\n",
        "        return tfidf, content_lowercase_dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntuSY9mWsHa1"
      },
      "source": [
        "### K-means clustering class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XDOlD4VBsHa1"
      },
      "outputs": [],
      "source": [
        "class KMeansAlgorithm:\n",
        "\n",
        "    def __init__(self, X, k, max_iterations=100):\n",
        "      self.X = X.values\n",
        "      self.k = k\n",
        "      self.max_iterations = max_iterations\n",
        "      self.centroids = None\n",
        "      self.num_articles = X.shape[0]\n",
        "\n",
        "    def init_random_centroids(self):\n",
        "      # Initializing KMeans by choosing random centroids\n",
        "      idx = np.random.choice(self.num_articles, size=self.k, replace=False) # Extract random indices from dataframe\n",
        "      centroids = self.X[idx] # Pick random rows from the dataframe\n",
        "      return centroids\n",
        "\n",
        "    def calculate_euclidean_distances(self):\n",
        "      # Calculate distances from vector/row to centroid\n",
        "      num_centroids = self.centroids.shape[0]\n",
        "      distances = np.zeros((num_centroids, self.num_articles))\n",
        "\n",
        "      for centroid_idx in range(num_centroids):\n",
        "          for article_idx in range(self.num_articles):\n",
        "              distances[centroid_idx, article_idx] = np.sqrt(np.sum((self.centroids[centroid_idx, :] - self.X[article_idx, :]) ** 2))\n",
        "      return distances\n",
        "\n",
        "    def update_centroids(self, labels):\n",
        "        # Calculate the mean of each cluster as new centroid\n",
        "        new_centroids = []\n",
        "        for k in range(self.k):\n",
        "            mean = self.X[labels == k].mean(axis=0)\n",
        "            new_centroids.append(mean)\n",
        "        new_centroids = np.array(new_centroids)\n",
        "        return new_centroids\n",
        "\n",
        "    def plot_clusters(self, labels, centroids, iteration):\n",
        "        # We will use PCA to plots clusters as the TFIDF matrix has many dimensions\n",
        "        unique_labels = np.unique(labels)\n",
        "        pca = PCA(n_components=3)\n",
        "        data_3d = pca.fit_transform(self.X)\n",
        "        centroids_3d = pca.transform(centroids)\n",
        "        # 3D plot\n",
        "        fig = plt.figure(figsize=(8, 6))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        color_map = cm.get_cmap('turbo', len(unique_labels))\n",
        "        # Plot data\n",
        "        for label in unique_labels:\n",
        "            indices = labels == label # like [1, 2, 1, 3, 1]\n",
        "            ax.scatter(data_3d[indices, 0], data_3d[indices, 1], data_3d[indices, 2], label=f'Cluster {label}', c=[color_map(label)])\n",
        "        # Plot centroids\n",
        "        ax.scatter(centroids_3d[:, 0], centroids_3d[:, 1], centroids_3d[:, 2], c='red', marker='x', s=80, label='Centroids')\n",
        "        # ax.set_title(f'PCA 3D - iteration {iteration}')\n",
        "        ax.set_xlabel('Principal component 1')\n",
        "        ax.set_ylabel('Principal component 2')\n",
        "        ax.set_zlabel('Principal component 3')\n",
        "        ax.legend(loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
        "        clear_output(wait=True)\n",
        "        plt.show()\n",
        "        fig.savefig('figures/PCA-plot.png', bbox_inches='tight', pad_inches=0.25)\n",
        "\n",
        "    def fit(self):\n",
        "      print(\"Clustering: KMeans fit()\", datetime.now().strftime(\"%H:%M:%S\"))\n",
        "\n",
        "      # Run the Kmeans algorithm using helper functions\n",
        "      self.centroids = self.init_random_centroids() # Init centroids\n",
        "\n",
        "      for i in range(self.max_iterations):\n",
        "        distances = self.calculate_euclidean_distances() # calculate eucledian distances\n",
        "        labels = np.argmin(distances, axis=0) # Assign to the cluster with the centroid that has the minimum distance to that point\n",
        "        new_centroids = self.update_centroids(labels) # Calculated centroids based on mean of the points in that cluster\n",
        "\n",
        "        if np.all(new_centroids == self.centroids): # If no new centroids break loop\n",
        "          print(\"Clustering: Kmeans has converged!\", datetime.now().strftime(\"%H:%M:%S\"))\n",
        "          break\n",
        "\n",
        "        self.centroids = new_centroids\n",
        "        self.plot_clusters(labels, self.centroids, i) # Plot PCA 3D plot\n",
        "\n",
        "      return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zihZSM4BsHa1"
      },
      "outputs": [],
      "source": [
        "def extract_documents(df):\n",
        "    \"\"\"\n",
        "    Main TF-IDF Function\n",
        "    Total time for 500 articles: 20 minutes\n",
        "\n",
        "    Args:\n",
        "        df (dataframe): dataframe with preprocessed content\n",
        "\n",
        "    Returns:\n",
        "        tf idf matrix\n",
        "    \"\"\"\n",
        "    tfidf = TFIDF(df)\n",
        "    tfidf_matrix, content_lowercase_dataframe = tfidf.fit()\n",
        "\n",
        "    return tfidf_matrix, content_lowercase_dataframe\n",
        "\n",
        "def Cluster_Articles(k, data):\n",
        "    \"\"\"Main cluster articles function\n",
        "\n",
        "    Args:\n",
        "        k (int): nr clusters\n",
        "        data (dataframe): PD dataframe with contents, etc.\n",
        "\n",
        "    Returns:\n",
        "        dataframe: df with ['clusters'] appended\n",
        "    \"\"\"\n",
        "    tfidf_matrix, dummy = extract_documents(data)\n",
        "    Kmeans = KMeansAlgorithm(tfidf_matrix, k) # Optimal number of clusters is determined from the results of the elbow method\n",
        "    labels = Kmeans.fit()\n",
        "    data['cluster'] = labels\n",
        "    return data\n",
        "\n",
        "def elbow_method(data):\n",
        "    # We will use elbow method to determine optimal number of clusters\n",
        "    num_clusters = range(1, 31)\n",
        "    wcss = []\n",
        "\n",
        "    for k in num_clusters:\n",
        "        # We will use Sklearn's Kmeans algorithm, but use random intialization as used in our implementation\n",
        "        kmeans = KMeans(n_clusters=k, init='random', random_state=123)\n",
        "        kmeans.fit(data)\n",
        "        wcss.append(kmeans.inertia_)\n",
        "\n",
        "    # Plot the Elbow Method\n",
        "    fig, ax = plt.subplots(figsize=(7, 4), tight_layout=True, sharey=True)\n",
        "    lines = plt.plot(num_clusters, wcss)\n",
        "    plt.setp(lines, color='tab:grey', linewidth=3.0, linestyle='-', marker='s', markerfacecolor='C0', markeredgecolor='k',  markersize=7.5)\n",
        "    # plt.annotate('Elbow', xy=(10, 86.25), xytext=(12, 90), arrowprops=dict(facecolor='black', shrink=0.05))\n",
        "    # plt.title('Elbow Method - KMeans with random initialization')\n",
        "    plt.xlabel('Number of clusters')\n",
        "    plt.ylabel('WCSS')\n",
        "    plt.grid(True)\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(2))\n",
        "    ax.xaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
        "    plt.show()\n",
        "    fig.savefig('figures/elbow-plot.png', bbox_inches='tight', pad_inches=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DDrY8IFIsHa2"
      },
      "outputs": [],
      "source": [
        "def generate_wordclouds(content_lowercase_dataframe, cluster_labels):\n",
        "    # Create dataframe with cluster numbers as columns and individual articles as rows\n",
        "    # The (lowercase) content of each article will be assigned to the corresponding row (article) and column (cluster) of the dataframe\n",
        "    # Remaining cells will be null\n",
        "    clusters_dataframe = pd.DataFrame()\n",
        "\n",
        "    list_to_append = [] # Row of null values that can be appended to dataframe\n",
        "\n",
        "    for i in range(len(np.unique(cluster_labels))):\n",
        "        clusters_dataframe[str(i)] = ''\n",
        "        list_to_append.append(np.nan)\n",
        "\n",
        "    # Looping through the cluster labels of all articles\n",
        "    for i in range(len(cluster_labels)):\n",
        "        # Initialize row with null values\n",
        "        clusters_dataframe.loc[len(clusters_dataframe.index)] = list_to_append\n",
        "        # Add article to the cluster it belongs to in the dataframe\n",
        "        clusters_dataframe.loc[i,str(cluster_labels[i])] = content_lowercase_dataframe.loc[i,'content_lowercase']\n",
        "\n",
        "    # display(clusters_dataframe.shape)\n",
        "\n",
        "    # This DataFrame has only one row, and the same number of columns as there are clusters.\n",
        "    # Every column contains ALL the text of a given cluster.\n",
        "    text_cluster_df = pd.DataFrame()\n",
        "\n",
        "    list_to_append = [] # Row of null values that can be appended to dataframe\n",
        "\n",
        "    for i in range(clusters_dataframe.shape[1]):\n",
        "        text_cluster_df[str(i)] = ''\n",
        "        list_to_append.append(np.nan)\n",
        "\n",
        "    # Initialize first row\n",
        "    text_cluster_df.loc[len(text_cluster_df.index)] = list_to_append\n",
        "\n",
        "    # text_cluster_df.head()\n",
        "\n",
        "    text = []\n",
        "    # Outer loop: The number of clusters\n",
        "    for i in range(clusters_dataframe.shape[1]):\n",
        "        # Inner loop: Number of articles\n",
        "        for j in range(clusters_dataframe.shape[0]):\n",
        "            if (pd.notnull(clusters_dataframe.loc[j,str(i)])):\n",
        "                # If the field is not null, then append the text to be added for the given cluster\n",
        "                text.append(clusters_dataframe.loc[j,str(i)])\n",
        "        text_cluster_df.loc[0, str(i)] =  ' '.join(text)\n",
        "        text = []\n",
        "\n",
        "    # WORDCLOUD\n",
        "    wordclouds = []\n",
        "    for i in range(text_cluster_df.shape[1]):\n",
        "        text = text_cluster_df.loc[0,str(i)]\n",
        "        # Create and generate a word cloud image:\n",
        "        wordcloud = WordCloud(max_font_size=50, max_words=15, background_color=\"white\").generate(text)\n",
        "        wordclouds.append(wordcloud)\n",
        "\n",
        "    plt.figure(figsize=(17, 17))\n",
        "    for n, cloud in enumerate(wordclouds):\n",
        "        # add a new subplot iteratively\n",
        "        ax = plt.subplot(7, 3, n + 1)\n",
        "        ax.imshow(cloud, interpolation=\"bilinear\")\n",
        "        ax.set_xlabel(str(n+1), fontsize=18)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "    plt.savefig('figures/wordclouds-nolib.png', bbox_inches='tight', pad_inches=0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBFxPAGvsHa2"
      },
      "source": [
        "### Load datasets to analyze\n",
        "- One or more datasets can be loaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FppbX7XsHa4"
      },
      "source": [
        "## Running TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsHEzmd2sHa4"
      },
      "source": [
        "### Process Similar Articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r690oh7TsHa4"
      },
      "source": [
        "### Determine number of clusters\n",
        "- The number of clusters is determined using the elbow method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text_lowercase_df(data):\n",
        "    articles = data.shape[0]\n",
        "    lowercase_list = []\n",
        "    for i in range(articles):\n",
        "        text = data.loc[i,'content']\n",
        "        text = text.lower()\n",
        "        # Converts to lowercase, removes symbols and stopword, but does not stem\n",
        "        # Used for wordclouds\n",
        "        # Remove special characters, digits and white space using regex expression\n",
        "        text = re.sub(r'[^a-z\\s]', '', text)\n",
        "        # Tokenize and remove stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in text.split() if word not in stop_words]\n",
        "        lowercase_article = ' '.join(tokens)\n",
        "        lowercase_list.append(lowercase_article)\n",
        "    lowercase_dataframe = pd.DataFrame()\n",
        "    lowercase_dataframe['content_lowercase'] = lowercase_list\n",
        "    return lowercase_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 20 entries in data (size 449):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>name</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>author</th>\n",
              "      <th>content</th>\n",
              "      <th>date</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "      <th>preprocessed_content</th>\n",
              "      <th>cluster</th>\n",
              "      <th>event</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>Two in custody after allegedly attacking Jewis...</td>\n",
              "      <td>https://nypost.com/2023/11/27/metro/2-women-ar...</td>\n",
              "      <td>Amanda Woods</td>\n",
              "      <td>Two women have been slapped with hate crime ch...</td>\n",
              "      <td>Nov. 27, 2023, 1:15 p.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>two women slap hate crime charg forallegedli a...</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>Hamas frees 11 more hostages as US hopes two A...</td>\n",
              "      <td>https://nypost.com/2023/11/27/news/hamas-set-t...</td>\n",
              "      <td>Ronny Reyes</td>\n",
              "      <td>Hamas freed11 more Israeli hostages— nine chil...</td>\n",
              "      <td>Nov. 27, 2023, 2:01 p.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>hama freed isra hostag nine children two mothe...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>Israeli military confirms the nearly dozen kid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>NBA star at risk of losing $40 million over al...</td>\n",
              "      <td>https://nypost.com/2023/11/27/sports/josh-gidd...</td>\n",
              "      <td>Jenna Lemoncelli</td>\n",
              "      <td>Oklahoma City Thunder guard Josh Giddey is re...</td>\n",
              "      <td>Nov. 27, 2023, 1:35 p.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>oklahoma citi thunder guard josh giddey report...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>Frosty the Joe-man: White House Christmas part...</td>\n",
              "      <td>https://nypost.com/2023/11/27/news/white-house...</td>\n",
              "      <td>Steven Nelson</td>\n",
              "      <td>WASHINGTON — A large number of White House rep...</td>\n",
              "      <td>Nov. 27, 2023, 3:09 p.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>washington larg number white hous report snub ...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>Dave Portnoy quits gambling again after NFL he...</td>\n",
              "      <td>https://nypost.com/2023/11/27/sports/dave-port...</td>\n",
              "      <td>Erich Richter</td>\n",
              "      <td>It is the end of an era. Notorious gambler an...</td>\n",
              "      <td>Nov. 27, 2023, 1:18 p.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>end era notori gambler barstool sport founder ...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>'Squad' silent after anti-Israel group points ...</td>\n",
              "      <td>https://nypost.com/2023/11/27/news/squad-silen...</td>\n",
              "      <td>Josh Christenson</td>\n",
              "      <td>A human rights organization with a long histor...</td>\n",
              "      <td>Nov. 27, 2023, 3:26 p.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>human right organ long histori antiisrael stat...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>Prince William has 'no plans' to see Harry aga...</td>\n",
              "      <td>https://nypost.com/2023/11/27/entertainment/pr...</td>\n",
              "      <td>Jack Hobbs</td>\n",
              "      <td>It’s a tale of two brothers. Royal expert Sara...</td>\n",
              "      <td>Nov. 27, 2023, 1:36 p.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>tale two brother royal expert sarah hewson cla...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>Disney employee racks up $24K on corporate car...</td>\n",
              "      <td>https://nypost.com/2023/11/27/business/disney-...</td>\n",
              "      <td>Alexandra Steigrad</td>\n",
              "      <td>A Disneyland employee who spent $24,000 on hi...</td>\n",
              "      <td>Nov. 27, 2023, 12:49 p.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>disneyland employe spent corpor credit card fu...</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>Common cleaner no longer strong enough to kill...</td>\n",
              "      <td>https://nypost.com/2023/11/27/lifestyle/common...</td>\n",
              "      <td>Alex Mitchell</td>\n",
              "      <td>It’s not good news. New research shows that so...</td>\n",
              "      <td>Nov. 27, 2023, 2:19 p.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>good news new research show use hospit cleaner...</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>Amazon's Cyber Monday blowout: 55+ incredible ...</td>\n",
              "      <td>https://nypost.com/shopping/amazon-cyber-monda...</td>\n",
              "      <td>Victoria Giardina</td>\n",
              "      <td>As you can imagine,Amazonhas a widespread col...</td>\n",
              "      <td>Nov. 27, 2023, 8:05 a.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>imagineamazonha widespread collect even that u...</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>Uncover Walmart's Cyber Monday treasures for e...</td>\n",
              "      <td>https://nypost.com/shopping/walmart-cyber-mond...</td>\n",
              "      <td>Nic Dobija-Nootens</td>\n",
              "      <td>This year, Walmart knocked it out of the park ...</td>\n",
              "      <td>Nov. 26, 2023, 7:27 p.m. ET</td>\n",
              "      <td>26</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>year walmart knock park black friday deal were...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>Experience smart living with the best extended...</td>\n",
              "      <td>https://nypost.com/shopping/best-cyber-monday-...</td>\n",
              "      <td>Kendall Cornish</td>\n",
              "      <td>It’s time to turn up the heat this Black Frida...</td>\n",
              "      <td>Nov. 27, 2023, 2:51 p.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>time turn heat black friday cyber monday weeke...</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>Get the best Cyber Monday fitness deals on hom...</td>\n",
              "      <td>https://nypost.com/shopping/best-cyber-monday-...</td>\n",
              "      <td>Nic Dobija-Nootens</td>\n",
              "      <td>You don’t have to wait until the new year to s...</td>\n",
              "      <td>Nov. 27, 2023, 4:30 a.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>dont wait new year start plan work fit goal fa...</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>Incredible savings: shop the best Cyber Monday...</td>\n",
              "      <td>https://nypost.com/shopping/apple-cyber-monday...</td>\n",
              "      <td>P.J. McCormick</td>\n",
              "      <td>It’s never a bad time to invest in a new Apple...</td>\n",
              "      <td>Nov. 27, 2023, 3:11 p.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>never bad time invest new appl product whether...</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>Whoopi Goldberg slams critics of Dolly Parton'...</td>\n",
              "      <td>https://nypost.com/2023/11/27/entertainment/wh...</td>\n",
              "      <td>Jack Hobbs</td>\n",
              "      <td>Bless their hearts. Whoopi Goldbergslammed cri...</td>\n",
              "      <td>Nov. 27, 2023, 3:08 p.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>bless heart whoopi goldbergslam critic dolli p...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>We put multiple best-selling pillows to the te...</td>\n",
              "      <td>https://nypost.com/2023/11/27/shopping/seven-s...</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Lying on your back, positioned on your side or...</td>\n",
              "      <td>Nov. 27, 2023, 12:01 a.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>lie back posit side stretch stomach everyon sl...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>Largest US journalist union rejects bid by pro...</td>\n",
              "      <td>https://nypost.com/2023/11/27/media/largest-us...</td>\n",
              "      <td>Alexandra Steigrad</td>\n",
              "      <td>Leading members of the country’s largest jour...</td>\n",
              "      <td>Nov. 27, 2023, 3:19 p.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>lead member countri largest journalist union h...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>Abigail Edan, 4-year-old hostage orphaned by H...</td>\n",
              "      <td>https://nypost.com/2023/11/27/news/abigail-eda...</td>\n",
              "      <td>Chris Nesi</td>\n",
              "      <td>Heartwarming photos show the first moments a 4...</td>\n",
              "      <td>Nov. 27, 2023, 1:22 p.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>heartwarm photo show first moment yearold isra...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>Red Cross says it has received 11 more Israeli...</td>\n",
              "      <td>https://nypost.com/2023/11/27/news/israel-hama...</td>\n",
              "      <td>Post Staff</td>\n",
              "      <td>Eleven more Hamas hostages have been freedand ...</td>\n",
              "      <td>Nov. 27, 2023, 3:36 p.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>eleven hama hostag freedand way israel red cro...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>Earlier Monday, the Biden administration said ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>NY Post</td>\n",
              "      <td>Southwest passenger arrested after escaping th...</td>\n",
              "      <td>https://nypost.com/2023/11/27/news/southwest-a...</td>\n",
              "      <td>Isabel Keane</td>\n",
              "      <td>A man was arrested after allegedly jumping out...</td>\n",
              "      <td>Nov. 27, 2023, 9:08 a.m. ET</td>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>2023</td>\n",
              "      <td>man arrest allegedli jump emerg hatch southwes...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Unnamed: 0     name                                              title  \\\n",
              "0            0  NY Post  Two in custody after allegedly attacking Jewis...   \n",
              "1            1  NY Post  Hamas frees 11 more hostages as US hopes two A...   \n",
              "2            2  NY Post  NBA star at risk of losing $40 million over al...   \n",
              "3            3  NY Post  Frosty the Joe-man: White House Christmas part...   \n",
              "4            4  NY Post  Dave Portnoy quits gambling again after NFL he...   \n",
              "5            5  NY Post  'Squad' silent after anti-Israel group points ...   \n",
              "6            6  NY Post  Prince William has 'no plans' to see Harry aga...   \n",
              "7            7  NY Post  Disney employee racks up $24K on corporate car...   \n",
              "8            8  NY Post  Common cleaner no longer strong enough to kill...   \n",
              "9            9  NY Post  Amazon's Cyber Monday blowout: 55+ incredible ...   \n",
              "10          10  NY Post  Uncover Walmart's Cyber Monday treasures for e...   \n",
              "11          11  NY Post  Experience smart living with the best extended...   \n",
              "12          12  NY Post  Get the best Cyber Monday fitness deals on hom...   \n",
              "13          13  NY Post  Incredible savings: shop the best Cyber Monday...   \n",
              "14          14  NY Post  Whoopi Goldberg slams critics of Dolly Parton'...   \n",
              "15          15  NY Post  We put multiple best-selling pillows to the te...   \n",
              "16          16  NY Post  Largest US journalist union rejects bid by pro...   \n",
              "17          17  NY Post  Abigail Edan, 4-year-old hostage orphaned by H...   \n",
              "18          18  NY Post  Red Cross says it has received 11 more Israeli...   \n",
              "19          19  NY Post  Southwest passenger arrested after escaping th...   \n",
              "\n",
              "                                                  url              author  \\\n",
              "0   https://nypost.com/2023/11/27/metro/2-women-ar...        Amanda Woods   \n",
              "1   https://nypost.com/2023/11/27/news/hamas-set-t...         Ronny Reyes   \n",
              "2   https://nypost.com/2023/11/27/sports/josh-gidd...    Jenna Lemoncelli   \n",
              "3   https://nypost.com/2023/11/27/news/white-house...       Steven Nelson   \n",
              "4   https://nypost.com/2023/11/27/sports/dave-port...       Erich Richter   \n",
              "5   https://nypost.com/2023/11/27/news/squad-silen...    Josh Christenson   \n",
              "6   https://nypost.com/2023/11/27/entertainment/pr...          Jack Hobbs   \n",
              "7   https://nypost.com/2023/11/27/business/disney-...  Alexandra Steigrad   \n",
              "8   https://nypost.com/2023/11/27/lifestyle/common...       Alex Mitchell   \n",
              "9   https://nypost.com/shopping/amazon-cyber-monda...   Victoria Giardina   \n",
              "10  https://nypost.com/shopping/walmart-cyber-mond...  Nic Dobija-Nootens   \n",
              "11  https://nypost.com/shopping/best-cyber-monday-...     Kendall Cornish   \n",
              "12  https://nypost.com/shopping/best-cyber-monday-...  Nic Dobija-Nootens   \n",
              "13  https://nypost.com/shopping/apple-cyber-monday...      P.J. McCormick   \n",
              "14  https://nypost.com/2023/11/27/entertainment/wh...          Jack Hobbs   \n",
              "15  https://nypost.com/2023/11/27/shopping/seven-s...             Unknown   \n",
              "16  https://nypost.com/2023/11/27/media/largest-us...  Alexandra Steigrad   \n",
              "17  https://nypost.com/2023/11/27/news/abigail-eda...          Chris Nesi   \n",
              "18  https://nypost.com/2023/11/27/news/israel-hama...          Post Staff   \n",
              "19  https://nypost.com/2023/11/27/news/southwest-a...        Isabel Keane   \n",
              "\n",
              "                                              content  \\\n",
              "0   Two women have been slapped with hate crime ch...   \n",
              "1   Hamas freed11 more Israeli hostages— nine chil...   \n",
              "2    Oklahoma City Thunder guard Josh Giddey is re...   \n",
              "3   WASHINGTON — A large number of White House rep...   \n",
              "4    It is the end of an era. Notorious gambler an...   \n",
              "5   A human rights organization with a long histor...   \n",
              "6   It’s a tale of two brothers. Royal expert Sara...   \n",
              "7    A Disneyland employee who spent $24,000 on hi...   \n",
              "8   It’s not good news. New research shows that so...   \n",
              "9    As you can imagine,Amazonhas a widespread col...   \n",
              "10  This year, Walmart knocked it out of the park ...   \n",
              "11  It’s time to turn up the heat this Black Frida...   \n",
              "12  You don’t have to wait until the new year to s...   \n",
              "13  It’s never a bad time to invest in a new Apple...   \n",
              "14  Bless their hearts. Whoopi Goldbergslammed cri...   \n",
              "15  Lying on your back, positioned on your side or...   \n",
              "16   Leading members of the country’s largest jour...   \n",
              "17  Heartwarming photos show the first moments a 4...   \n",
              "18  Eleven more Hamas hostages have been freedand ...   \n",
              "19  A man was arrested after allegedly jumping out...   \n",
              "\n",
              "                            date day month  year  \\\n",
              "0    Nov. 27, 2023, 1:15 p.m. ET  27    11  2023   \n",
              "1    Nov. 27, 2023, 2:01 p.m. ET  27    11  2023   \n",
              "2    Nov. 27, 2023, 1:35 p.m. ET  27    11  2023   \n",
              "3    Nov. 27, 2023, 3:09 p.m. ET  27    11  2023   \n",
              "4    Nov. 27, 2023, 1:18 p.m. ET  27    11  2023   \n",
              "5    Nov. 27, 2023, 3:26 p.m. ET  27    11  2023   \n",
              "6    Nov. 27, 2023, 1:36 p.m. ET  27    11  2023   \n",
              "7   Nov. 27, 2023, 12:49 p.m. ET  27    11  2023   \n",
              "8    Nov. 27, 2023, 2:19 p.m. ET  27    11  2023   \n",
              "9    Nov. 27, 2023, 8:05 a.m. ET  27    11  2023   \n",
              "10   Nov. 26, 2023, 7:27 p.m. ET  26    11  2023   \n",
              "11   Nov. 27, 2023, 2:51 p.m. ET  27    11  2023   \n",
              "12   Nov. 27, 2023, 4:30 a.m. ET  27    11  2023   \n",
              "13   Nov. 27, 2023, 3:11 p.m. ET  27    11  2023   \n",
              "14   Nov. 27, 2023, 3:08 p.m. ET  27    11  2023   \n",
              "15  Nov. 27, 2023, 12:01 a.m. ET  27    11  2023   \n",
              "16   Nov. 27, 2023, 3:19 p.m. ET  27    11  2023   \n",
              "17   Nov. 27, 2023, 1:22 p.m. ET  27    11  2023   \n",
              "18   Nov. 27, 2023, 3:36 p.m. ET  27    11  2023   \n",
              "19   Nov. 27, 2023, 9:08 a.m. ET  27    11  2023   \n",
              "\n",
              "                                 preprocessed_content cluster  event  \\\n",
              "0   two women slap hate crime charg forallegedli a...       6      0   \n",
              "1   hama freed isra hostag nine children two mothe...       0      2   \n",
              "2   oklahoma citi thunder guard josh giddey report...       5      0   \n",
              "3   washington larg number white hous report snub ...       0      3   \n",
              "4   end era notori gambler barstool sport founder ...       5      0   \n",
              "5   human right organ long histori antiisrael stat...       0      0   \n",
              "6   tale two brother royal expert sarah hewson cla...       2      0   \n",
              "7   disneyland employe spent corpor credit card fu...       6      0   \n",
              "8   good news new research show use hospit cleaner...      10      0   \n",
              "9   imagineamazonha widespread collect even that u...       6      0   \n",
              "10  year walmart knock park black friday deal were...       5      0   \n",
              "11  time turn heat black friday cyber monday weeke...       9      0   \n",
              "12  dont wait new year start plan work fit goal fa...       9      0   \n",
              "13  never bad time invest new appl product whether...       6      0   \n",
              "14  bless heart whoopi goldbergslam critic dolli p...       3      0   \n",
              "15  lie back posit side stretch stomach everyon sl...       3      0   \n",
              "16  lead member countri largest journalist union h...       1      0   \n",
              "17  heartwarm photo show first moment yearold isra...       0      0   \n",
              "18  eleven hama hostag freedand way israel red cro...       0      2   \n",
              "19  man arrest allegedli jump emerg hatch southwes...       1      0   \n",
              "\n",
              "                                              summary  \n",
              "0                                                   -  \n",
              "1   Israeli military confirms the nearly dozen kid...  \n",
              "2                                                   -  \n",
              "3                                                   -  \n",
              "4                                                   -  \n",
              "5                                                   -  \n",
              "6                                                   -  \n",
              "7                                                   -  \n",
              "8                                                   -  \n",
              "9                                                   -  \n",
              "10                                                  -  \n",
              "11                                                  -  \n",
              "12                                                  -  \n",
              "13                                                  -  \n",
              "14                                                  -  \n",
              "15                                                  -  \n",
              "16                                                  -  \n",
              "17                                                  -  \n",
              "18  Earlier Monday, the Biden administration said ...  \n",
              "19                                                  -  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# dataset_paths = [file for file in os.listdir(\"./data/\") if file.endswith(('csv'))] # Select all available data files\n",
        "# dataset_paths = [\"scraping_data_filtered.csv\"]\n",
        "dataset_paths = [\"Monday_new_scraping_data_filtered_clustered11.csv\"]\n",
        "# dataset_paths = [\"articles1.csv\"]\n",
        "\n",
        "# Load data\n",
        "data_frames = []\n",
        "data = []\n",
        "DATA_LIMIT = 500 # how many items to load\n",
        "for data_path in dataset_paths:\n",
        "    data_frames.append(pd.read_csv(os.path.join('./data/', data_path)).head(DATA_LIMIT))\n",
        "data = pd.concat(data_frames)\n",
        "\n",
        "\n",
        "# data_new = data.drop(['Unnamed: 0', 'date', 'url', 'cluster', 'event', 'summary'], axis=1, errors='ignore')\n",
        "\n",
        "if len(data) > 0:\n",
        "        # display dataframe header\n",
        "        print(\"First 20 entries in data (size \" + str(len(data)) + \"):\")\n",
        "        display(data.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(449, 14)\n",
            "(440, 14)\n",
            "(440, 8)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "numpy.int32"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# data = pd.read_csv(\"data/Monday_new_scraping_data_filtered_clustered11.csv\")\n",
        "\n",
        "# data['content'] = data['content'].astype(str)\n",
        "# print(data.shape)\n",
        "# # Convert the column to numeric, errors='coerce' will replace non-numeric values with NaN\n",
        "# data['cluster'] = pd.to_numeric(data['cluster'], errors='coerce')\n",
        "\n",
        "# # Drop rows with NaN values in the specified column\n",
        "# data = data.dropna(subset=['cluster'])\n",
        "\n",
        "# # Convert the column to integer type\n",
        "# data['cluster'] = data['cluster'].astype(int)\n",
        "# print(data.shape)\n",
        "# data_temp = data.drop(['Unnamed: 0', 'date', 'url', 'cluster', 'event', 'summary'], axis=1, errors='ignore')\n",
        "# print(data_temp.shape)\n",
        "# # display(type(data))\n",
        "# # display(type(data_temp.copy()))\n",
        "# display(type(data.loc[0,'cluster']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lowercase_content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>two women slapped hate crime charges forallege...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hamas freed israeli hostages nine children two...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>oklahoma city thunder guard josh giddey report...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>washington large number white house reporters ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>end era notorious gambler barstool sports foun...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   lowercase_content\n",
              "0  two women slapped hate crime charges forallege...\n",
              "1  hamas freed israeli hostages nine children two...\n",
              "2  oklahoma city thunder guard josh giddey report...\n",
              "3  washington large number white house reporters ...\n",
              "4  end era notorious gambler barstool sports foun..."
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['content'] = data['content'].astype(str)\n",
        "content_lowercase_dataframe = pd.DataFrame()\n",
        "content_lowercase_dataframe = preprocess_text_lowercase_df(data)\n",
        "display(content_lowercase_dataframe.head())\n",
        "print(content_lowercase_dataframe.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the column to numeric, errors='coerce' will replace non-numeric values with NaN\n",
        "data['cluster'] = pd.to_numeric(data['cluster'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN values in the specified column\n",
        "data = data.dropna(subset=['cluster'])\n",
        "\n",
        "# Convert the column to integer type\n",
        "data['cluster'] = data['cluster'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oNLzIwrsHa4",
        "outputId": "594410be-5933-44ec-af66-537682a29292"
      },
      "outputs": [],
      "source": [
        "# tfidf_dataframe, content_lowercase_dataframe = extract_documents(data_temp.copy())\n",
        "\n",
        "# elbow_method(tfidf_dataframe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_8wQfwVNsHa5",
        "outputId": "30622c94-2939-4038-a024-286ea5cadd0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 20 entries in data with clusters:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Two in custody after allegedly attacking Jewis...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hamas frees 11 more hostages as US hopes two A...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NBA star at risk of losing $40 million over al...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Frosty the Joe-man: White House Christmas part...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dave Portnoy quits gambling again after NFL he...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  cluster\n",
              "0  Two in custody after allegedly attacking Jewis...        6\n",
              "1  Hamas frees 11 more hostages as US hopes two A...        0\n",
              "2  NBA star at risk of losing $40 million over al...        5\n",
              "3  Frosty the Joe-man: White House Christmas part...        0\n",
              "4  Dave Portnoy quits gambling again after NFL he...        5"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "nr_clusters = 11 # Number of clusters\n",
        "\n",
        "# clustered_data = pd.DataFrame()\n",
        "\n",
        "# data_temp = Cluster_Articles(k=int(nr_clusters), data=data_temp.copy())\n",
        "\n",
        "cluster_labels = data['cluster'].to_numpy()\n",
        "\n",
        "if 'cluster' in data:\n",
        "    # display dataframe header\n",
        "    print(\"First 20 entries in data with clusters:\")\n",
        "    display(data[['title', 'cluster']].head(5))\n",
        "\n",
        "    # Save clusters\n",
        "    # clustered_path = text_area(\"Path\", value=\"clustered_articles1.csv\")\n",
        "    # data.to_csv(os.path.join(\"data/\", str(clustered_path)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.int32"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(data.loc[2,'cluster'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 6,  0,  5,  0,  5,  0,  2,  6, 10,  6,  5,  9,  9,  6,  3,  3,  1,\n",
              "        0,  0,  1,  5,  6,  4,  9,  7,  7,  9,  7,  7,  4,  1,  7,  0,  1,\n",
              "        9,  6,  4,  6, 10,  9,  9,  9,  5,  9,  6,  5, 10,  8,  4,  5,  3,\n",
              "        7,  9,  4,  1,  6,  5,  5,  6,  6,  5,  9,  6,  5,  5,  9,  0,  9,\n",
              "        7,  0,  5,  9, 10,  1, 10,  5,  9,  1,  3,  9,  8,  8,  1,  4,  0,\n",
              "        6,  5,  7,  6,  4,  0,  9,  4,  8,  8,  3,  5,  4,  5, 10,  1,  6,\n",
              "       10,  5,  8,  6,  8,  9, 10,  9,  4,  3,  9,  5,  9,  0,  0,  0,  0,\n",
              "        0,  0,  4,  9,  0,  9,  9,  4,  5,  1,  5,  0,  1,  6,  3,  9,  9,\n",
              "        8,  9,  4,  8,  6,  6,  4,  1,  3,  5,  6,  6,  6,  6,  4,  1,  9,\n",
              "        9,  9,  3,  6,  5,  5,  5,  7,  9,  8,  8,  1,  6,  1,  8,  0,  9,\n",
              "        9,  6,  9,  0,  9,  0,  1,  4,  9,  6,  4,  3,  9,  1,  6,  3,  8,\n",
              "        5,  9,  6,  8,  5,  9,  5,  5,  3,  3,  1,  3,  8,  3,  1,  9,  9,\n",
              "        4,  9,  3,  4,  9,  3,  4,  6,  9,  9,  3, 10,  8,  9,  3,  6,  6,\n",
              "        7, 10,  3,  9,  9,  4,  6,  6,  9,  9,  2,  5,  5,  9,  0,  9,  2,\n",
              "        2,  0,  2,  6, 10,  0,  2,  6,  6,  5,  5,  7,  9,  0,  9,  0,  0,\n",
              "        1,  1,  6,  9, 10, 10,  9, 10,  6, 10,  6,  6, 10,  9,  4,  4,  4,\n",
              "        4,  6,  6,  3,  3,  8,  1,  5, 10, 10, 10,  8,  1,  8,  3, 10,  8,\n",
              "        9,  2,  4,  1,  8,  4,  3,  7,  7,  5,  5,  6,  5,  0,  5,  0,  1,\n",
              "        1,  1,  5,  1,  1,  1,  6,  4,  5,  1,  9,  0,  0,  1,  0,  7,  0,\n",
              "        9,  3,  1,  1,  7,  1,  6,  1,  1,  1,  9,  1,  9,  6,  7,  3,  2,\n",
              "        1,  9,  7,  1,  0,  1,  9,  1,  7,  1,  0,  1,  3,  2,  5,  9,  3,\n",
              "        1,  1,  1,  1,  1,  1,  6, 10,  1,  9,  9,  1,  4,  0,  1,  0,  1,\n",
              "        1,  0,  0,  1,  3,  1,  1,  0,  0,  1,  1, 10, 10,  1,  9,  1,  4,\n",
              "        9,  1,  1,  4,  1,  1,  7,  7,  1,  9,  1,  3, 10,  4,  4,  4,  4,\n",
              "        9,  6,  8,  5,  1,  1,  1,  1,  1,  6,  1,  8,  1,  1,  1,  1,  1,\n",
              "        4,  4,  9,  3,  1,  5,  6,  1,  1,  1,  1,  1,  1,  1,  1])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(cluster_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ_dyRdVsHa5"
      },
      "source": [
        "### Generating wordclouds for each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uL5XnEccsHa5",
        "outputId": "012092a4-b13e-4468-8bb0-b3fc9ebd0652"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'content_lowercase'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\arime\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'content_lowercase'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32mc:\\MyDocs\\DTU\\MSc\\Courses\\Computational Tools for Data Science\\Project\\github\\NewsComparison\\wordclouds-from-clustered-data.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/MyDocs/DTU/MSc/Courses/Computational%20Tools%20for%20Data%20Science/Project/github/NewsComparison/wordclouds-from-clustered-data.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m generate_wordclouds(content_lowercase_dataframe, cluster_labels)\n",
            "\u001b[1;32mc:\\MyDocs\\DTU\\MSc\\Courses\\Computational Tools for Data Science\\Project\\github\\NewsComparison\\wordclouds-from-clustered-data.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/MyDocs/DTU/MSc/Courses/Computational%20Tools%20for%20Data%20Science/Project/github/NewsComparison/wordclouds-from-clustered-data.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     clusters_dataframe\u001b[39m.\u001b[39mloc[\u001b[39mlen\u001b[39m(clusters_dataframe\u001b[39m.\u001b[39mindex)] \u001b[39m=\u001b[39m list_to_append\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/MyDocs/DTU/MSc/Courses/Computational%20Tools%20for%20Data%20Science/Project/github/NewsComparison/wordclouds-from-clustered-data.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# Add article to the cluster it belongs to in the dataframe\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/MyDocs/DTU/MSc/Courses/Computational%20Tools%20for%20Data%20Science/Project/github/NewsComparison/wordclouds-from-clustered-data.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     clusters_dataframe\u001b[39m.\u001b[39mloc[i,\u001b[39mstr\u001b[39m(cluster_labels[i])] \u001b[39m=\u001b[39m content_lowercase_dataframe\u001b[39m.\u001b[39;49mloc[i,\u001b[39m'\u001b[39;49m\u001b[39mcontent_lowercase\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/MyDocs/DTU/MSc/Courses/Computational%20Tools%20for%20Data%20Science/Project/github/NewsComparison/wordclouds-from-clustered-data.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# display(clusters_dataframe.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/MyDocs/DTU/MSc/Courses/Computational%20Tools%20for%20Data%20Science/Project/github/NewsComparison/wordclouds-from-clustered-data.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/MyDocs/DTU/MSc/Courses/Computational%20Tools%20for%20Data%20Science/Project/github/NewsComparison/wordclouds-from-clustered-data.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# This DataFrame has only one row, and the same number of columns as there are clusters.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/MyDocs/DTU/MSc/Courses/Computational%20Tools%20for%20Data%20Science/Project/github/NewsComparison/wordclouds-from-clustered-data.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Every column contains ALL the text of a given cluster.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/MyDocs/DTU/MSc/Courses/Computational%20Tools%20for%20Data%20Science/Project/github/NewsComparison/wordclouds-from-clustered-data.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m text_cluster_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n",
            "File \u001b[1;32mc:\\Users\\arime\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1146\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(com\u001b[39m.\u001b[39mapply_if_callable(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m key)\n\u001b[0;32m   1145\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m-> 1146\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_get_value(\u001b[39m*\u001b[39;49mkey, takeable\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_takeable)\n\u001b[0;32m   1147\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_tuple(key)\n\u001b[0;32m   1148\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\arime\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4005\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   4002\u001b[0m     series \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ixs(col, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m   4003\u001b[0m     \u001b[39mreturn\u001b[39;00m series\u001b[39m.\u001b[39m_values[index]\n\u001b[1;32m-> 4005\u001b[0m series \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_item_cache(col)\n\u001b[0;32m   4006\u001b[0m engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_engine\n\u001b[0;32m   4008\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m   4009\u001b[0m     \u001b[39m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[0;32m   4010\u001b[0m     \u001b[39m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[0;32m   4011\u001b[0m     \u001b[39m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\arime\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4414\u001b[0m, in \u001b[0;36mDataFrame._get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   4409\u001b[0m res \u001b[39m=\u001b[39m cache\u001b[39m.\u001b[39mget(item)\n\u001b[0;32m   4410\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   4411\u001b[0m     \u001b[39m# All places that call _get_item_cache have unique columns,\u001b[39;00m\n\u001b[0;32m   4412\u001b[0m     \u001b[39m#  pending resolution of GH#33047\u001b[39;00m\n\u001b[1;32m-> 4414\u001b[0m     loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(item)\n\u001b[0;32m   4415\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ixs(loc, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m   4417\u001b[0m     cache[item] \u001b[39m=\u001b[39m res\n",
            "File \u001b[1;32mc:\\Users\\arime\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'content_lowercase'"
          ]
        }
      ],
      "source": [
        "generate_wordclouds(content_lowercase_dataframe, cluster_labels)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
